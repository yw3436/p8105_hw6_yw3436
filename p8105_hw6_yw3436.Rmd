---
title: "Homework 6"
author: Yuqi Wang
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1
```{r}
homicide_df = read_csv(file = "./data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    victim_age = as.numeric(victim_age),
    resolved = case_when(
      disposition  == "Closed without arrest" ~ "0",
      disposition  == "Open/No arrest" ~ "0",
      disposition  == "Closed by arrest" ~ "1",
    )
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa_AL") %>% 
  select(city_state, resolved, victim_age, victim_race, victim_sex)
```

start with one city:
```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore_MD") %>% 
  mutate(resolved = as.numeric(resolved))

glm(resolved ~ victim_age + victim_sex,
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96*std.error),
    CI_upper = exp(estimate + 1.96*std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```

Try this accross cities:
```{r}
model_results = homicide_df %>%
  mutate(resolved = as.numeric(resolved)) %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())), 
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96*std.error),
    CI_upper = exp(estimate + 1.96*std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI"))
```

Make a plot

```{r}
model_results %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper))
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


## Problem 2

First, read the data and check all variable type.
```{r}
baby_df = read_csv("./data/birthweight.csv") %>% 
  mutate(
    babysex = case_when(
      babysex == "1" ~ "male",
      babysex == "2" ~ "female",
      TRUE ~ as.character(babysex)
    ),
    frace = case_when(
      frace == "1" ~ "white",
      frace == "2" ~ "black",
      frace == "3" ~ "Asian",
      frace == "4" ~ "Puerto Rican",
      frace == "8" ~ "Other",
      frace == "9" ~ "Unknown",
      TRUE ~ as.character(frace)
    ),
    malform = case_when(
      malform == "0" ~ "absent",
      malform == "1" ~ "present",
      TRUE ~ as.character(malform)
    ),
    mrace = case_when(
      mrace == "1" ~ "white",
      mrace == "2" ~ "black",
      mrace == "3" ~ "Asian",
      mrace == "4" ~ "Puerto Rican",
      mrace == "8" ~ "Other",
      mrace == "9" ~ "Unknown",
      TRUE ~ as.character(mrace)
    ))

baby_df %>% 
  count(pnumlbw)
baby_df %>% 
  count(pnumsga)

```


Second, use common knowledge and the backward modeling method to find the appropriate the model.

For the outcome variable, the following plot shows that the birthweight is about normally distributed, so we will use linear regression.

```{r}
baby_df %>% 
  ggplot(aes(x = bwt)) + 
  geom_histogram()
```

Since we have the ppbmi variable, we will exclude the mother's height and pre-pregnancy weight in the model to avoid over-adjustment. Also, since wtgain was calculated by delwt and ppwt, we will remove delwt. Additionally, pnumlbw and pnumgsa are two variables that seem to be closely correlated, and all participants have 0 for the two variables, we will remove the two from the model.

Then, I will add all the other variables into the model for backward selection.

```{r}
model_naive = lm(bwt ~ babysex + bhead + blength + fincome + frace + gaweeks + malform + menarche + momage + mrace + parity + ppbmi + smoken + wtgain, data = baby_df)

model_naive %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)
```

After comparing the P-values with the pre-set alpha of 0.05, we will include `babysex`, `bhead`, `blength`, `fincome`, `gaweeks`, `parity`, `ppbmi`, `smoken` and `wtgain` into our model to predict baby birthweight.

Next, we will draw a plot for the residuals against the fitted values.
```{r}
model_1 = lm(bwt ~ babysex + bhead + blength + fincome + gaweeks + parity + ppbmi + smoken + wtgain, data = baby_df) 

baby_df %>% 
  add_predictions(model_1, var = "predicted") %>% 
  add_residuals(model_1, var = "resid") %>% 
  ggplot(aes(x = predicted, y = resid))+
  geom_point()
```

The scatter plot shows that for most observations, the residuals are not heavily skewed and there is no clear trend between the fitted value and the residuals when the predicted birthweight is larger than 2000g. However, when the predicted values are smaller than 2000, there seems to be a decreasing trend of the residuals, which we may consider as outliers.

#### Compare my model with the other two models using cross validation

```{r}
model_2 = lm(bwt ~ blength + gaweeks, data = baby_df)
model_3 = lm(bwt ~ bhead * blength * babysex, data = baby_df)
```

```{r}
cv_df = 
  crossv_mc(baby_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>% 
  mutate(
    model_1 = map(train, ~lm(bwt ~ babysex + bhead + blength + fincome + gaweeks + parity + ppbmi + smoken + wtgain, data = .x)),
    model_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_3 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y)),
  )

rmse_df = cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") 

rmse_df %>% 
  group_by(model) %>% 
  summarize(avg_rmse = mean(rmse)) %>% 
  knitr::kable()

rmse_df %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

```

According to the average RMSE value and the violin plot, the first model (with `babysex`, `bhead`, `blength`, `fincome`, `gaweeks`, `parity`, `ppbmi`, `smoken` and `wtgain`) is the best as it have the lowest RMSE. The model with `bhead`, `blength` `babysex`, and all the interactions is the second fitted model, while the model with only `blength` and `gaweeks` are the worst one.

We would like to use the first model for future analysis and prediction.

## Problem 3

First, load the data.
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Second, conduct bootstrapping and calculate the estimated R square and log(beta0*beta1).

```{r}
boot_straps = 
  weather_df %>% 
  modelr::bootstrap(n = 5000)  %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    betas = map(models, broom::tidy),
    rsq = map(models, broom::glance)) 

strap_df = boot_straps %>% 
  select(-strap, -models) %>% 
  unnest(betas, rsq) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    log_product = log(intercept*tmin)
  )
```

Third, make the plot to show the distribution of the estimated R square and log(beta0*beta1).

```{r}
strap_df %>% 
  ggplot(aes(x = r_squared)) + 
  geom_density()

strap_df %>% 
  ggplot(aes(x = log_product)) + 
  geom_density()
```

For the distribution of the r-squares, it is a slightly left-skewed distribution with a peak of 0.915.

For the distribution of the log(beta0*beta1), the distribution is also slightly left-skewed, with a peak of 2.02.

Though we have 5000 estimates in total the distribution for the two estimates doesn't follow exact normal distribution.

Identify the 95%CI for the two estimates:

```{r}
strap_df %>% 
  summarize(
    r_ci_lower = quantile(r_squared, 0.025), 
    r_ci_upper = quantile(r_squared, 0.975),
    log_ci_lower = quantile(log_product, 0.025), 
    log_ci_upper = quantile(log_product, 0.975),
    ) %>% 
  knitr::kable(digit = 3)
```

The result shows that the 95%CI for r-square is (0.894, 0.927), and for log(beta0*beta1) is (1.965, 2.059).

